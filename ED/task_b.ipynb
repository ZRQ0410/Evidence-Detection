{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3JnMM97qBEz9"
   },
   "source": [
    "# b)  Deep learning-based approaches that do not employ transformer architectures\n",
    "\n",
    "The final model used is Bi-directional GRU and GloVe embedding.\n",
    "\n",
    "Link to the model:https://drive.google.com/file/d/1WC6CifsnJe2Xs8NFO86beBxayjRTLt-a/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CSdlewJSwCoj"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, GRU, Embedding, Dense, concatenate, Dropout, Bidirectional, Attention\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "npHtOGAT63iF"
   },
   "source": [
    "### Read Data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K0nFzIT765vb"
   },
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('train.csv')\n",
    "dev_data = pd.read_csv('dev.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ibHRi7P6-EF"
   },
   "source": [
    "### Data Preprocess ###\n",
    "1. Load Tokenizer\n",
    "2. Convert the text to a sequence of integers\n",
    "3. Pad all sequences to make them the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BK3ffver6-W8"
   },
   "outputs": [],
   "source": [
    "# set as the max length of all sentences in training data\n",
    "max_len = 307\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(list(training_data['Claim']) + list(training_data['Evidence']))\n",
    "\n",
    "# Convert text to sequences of integers\n",
    "train_claim_seq = tokenizer.texts_to_sequences(training_data['Claim'])\n",
    "train_evidence_seq = tokenizer.texts_to_sequences(training_data['Evidence'])\n",
    "dev_claim_seq = tokenizer.texts_to_sequences(dev_data['Claim'])\n",
    "dev_evidence_seq = tokenizer.texts_to_sequences(dev_data['Evidence'])\n",
    "\n",
    "# Pad sequences to make the vector has the same length\n",
    "train_claim_seq = pad_sequences(train_claim_seq, maxlen=max_len)\n",
    "train_evidence_seq = pad_sequences(train_evidence_seq, maxlen=max_len)\n",
    "dev_claim_seq = pad_sequences(dev_claim_seq, maxlen=max_len)\n",
    "dev_evidence_seq = pad_sequences(dev_evidence_seq, maxlen=max_len)\n",
    "\n",
    "train_labels = training_data['label'].values\n",
    "dev_labels = dev_data['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w80-aP-w8TpH"
   },
   "source": [
    "### Word Embedding ###\n",
    "1. Upload the GloVe embedding file: https://drive.google.com/file/d/1MZnzxd-XPZd7kGS4iE1KmLYQWUGHfpOP/view?usp=sharing\n",
    "2. Create the embedding matrix based on the word of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "huoZswpHwLbE"
   },
   "outputs": [],
   "source": [
    "def load_glove_embeddings(path):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = vector\n",
    "    return embeddings_index\n",
    "\n",
    "# Path to the GloVe embeddings file\n",
    "glove_path = 'glove.6B.100d.txt'\n",
    "embeddings_index = load_glove_embeddings(glove_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G4syW6QBwO5S"
   },
   "outputs": [],
   "source": [
    "# Set the dimension of embedding as the GloVe file\n",
    "embedding_dim = 100\n",
    "\n",
    "# The size of vocabulary\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Create embedding matrix using zeros as start value\n",
    "# Add the embedding one by one\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i < vocab_size:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jy7rptjT9Htd"
   },
   "source": [
    "### Build Bi-GRU Model  ###\n",
    "\n",
    "The process and accuracy of each step of building the model\n",
    "1. One layer of GRU without word embedding: 77.17%\n",
    "2. Two layers of Bidirectional GRU and an Attention layer without word embedding: 79.26%\n",
    "3. Two layers of Bidirectional GRU and an Attention layer with GloVe word embedding: 81.20%\n",
    "4. HyperParameters Tuning based on previous model: 83.13%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1869,
     "status": "ok",
     "timestamp": 1713832338172,
     "user": {
      "displayName": "Yaqi Z",
      "userId": "12487690213530075120"
     },
     "user_tz": -60
    },
    "id": "zqM_MCPhwV-A",
    "outputId": "f5e59fb0-32dd-4e4b-c6b7-c74ba15473e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)        [(None, 307)]                0         []                            \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)        [(None, 307)]                0         []                            \n",
      "                                                                                                  \n",
      " embedding_4 (Embedding)     (None, 307, 100)             4088400   ['input_5[0][0]']             \n",
      "                                                                                                  \n",
      " embedding_5 (Embedding)     (None, 307, 100)             4088400   ['input_6[0][0]']             \n",
      "                                                                                                  \n",
      " bidirectional_8 (Bidirecti  (None, 307, 256)             176640    ['embedding_4[0][0]']         \n",
      " onal)                                                                                            \n",
      "                                                                                                  \n",
      " bidirectional_10 (Bidirect  (None, 307, 256)             176640    ['embedding_5[0][0]']         \n",
      " ional)                                                                                           \n",
      "                                                                                                  \n",
      " bidirectional_9 (Bidirecti  (None, 256)                  296448    ['bidirectional_8[0][0]']     \n",
      " onal)                                                                                            \n",
      "                                                                                                  \n",
      " bidirectional_11 (Bidirect  (None, 256)                  296448    ['bidirectional_10[0][0]']    \n",
      " ional)                                                                                           \n",
      "                                                                                                  \n",
      " attention_2 (Attention)     (None, 256)                  0         ['bidirectional_9[0][0]',     \n",
      "                                                                     'bidirectional_11[0][0]']    \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate  (None, 768)                  0         ['bidirectional_9[0][0]',     \n",
      " )                                                                   'bidirectional_11[0][0]',    \n",
      "                                                                     'attention_2[0][0]']         \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 128)                  98432     ['concatenate_2[0][0]']       \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 128)                  0         ['dense_4[0][0]']             \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (None, 1)                    129       ['dropout_2[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9221537 (35.18 MB)\n",
      "Trainable params: 1044737 (3.99 MB)\n",
      "Non-trainable params: 8176800 (31.19 MB)\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Input layers\n",
    "claim_input = Input(shape=(max_len,), dtype='int32')\n",
    "evidence_input = Input(shape=(max_len,), dtype='int32')\n",
    "\n",
    "# Embedding layer\n",
    "claim_emb = Embedding(input_dim=vocab_size, output_dim=embedding_dim, embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix), input_length=max_len, trainable=False)(claim_input)\n",
    "evidence_emb = Embedding(input_dim=vocab_size, output_dim=embedding_dim, embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix), input_length=max_len, trainable=False)(evidence_input)\n",
    "\n",
    "# Separate Bidirectional GRU layers of claims and evidence\n",
    "# Second Bidirectional GRU layer is to condense information at the output\n",
    "claim_gru = Bidirectional(GRU(units=128, return_sequences=True))(claim_emb)\n",
    "claim_gru = Bidirectional(GRU(units=128))(claim_gru)\n",
    "\n",
    "evidence_gru = Bidirectional(GRU(units=128, return_sequences=True))(evidence_emb)\n",
    "evidence_gru = Bidirectional(GRU(units=128))(evidence_gru)\n",
    "\n",
    "# Attention layer to focus on important parts of the claim and evidence\n",
    "attention_out = Attention()([claim_gru, evidence_gru])\n",
    "\n",
    "# Concatenation layer\n",
    "combined_features = concatenate([claim_gru, evidence_gru, attention_out])\n",
    "\n",
    "# Dense layer\n",
    "dense_layer = Dense(128, activation='relu')(combined_features)\n",
    "\n",
    "# Dropout layer to prevent overfitting\n",
    "dropout_layer = Dropout(0.7)(dense_layer)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(1, activation='sigmoid')(dropout_layer)  # Use 'sigmoid' for binary classification; 'softmax' for multi-class\n",
    "\n",
    "# Build and compile the model\n",
    "model = Model(inputs=[claim_input, evidence_input], outputs=output)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the architecture of model\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ox5FzBNY9dXA"
   },
   "source": [
    "### Train the model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 299049,
     "status": "ok",
     "timestamp": 1713832666382,
     "user": {
      "displayName": "Yaqi Z",
      "userId": "12487690213530075120"
     },
     "user_tz": -60
    },
    "id": "hhhMttJNwb4Y",
    "outputId": "13ad9f6a-4941-45fc-a0aa-441a77265aca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "593/593 [==============================] - 73s 101ms/step - loss: 0.4685 - accuracy: 0.7736 - val_loss: 0.4057 - val_accuracy: 0.8093\n",
      "Epoch 2/5\n",
      "593/593 [==============================] - 56s 95ms/step - loss: 0.4139 - accuracy: 0.8020 - val_loss: 0.4086 - val_accuracy: 0.8070\n",
      "Epoch 3/5\n",
      "593/593 [==============================] - 56s 95ms/step - loss: 0.3856 - accuracy: 0.8181 - val_loss: 0.3827 - val_accuracy: 0.8140\n",
      "Epoch 4/5\n",
      "593/593 [==============================] - 57s 96ms/step - loss: 0.3659 - accuracy: 0.8285 - val_loss: 0.3770 - val_accuracy: 0.8163\n",
      "Epoch 5/5\n",
      "593/593 [==============================] - 56s 94ms/step - loss: 0.3462 - accuracy: 0.8387 - val_loss: 0.3729 - val_accuracy: 0.8256\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([train_claim_seq, train_evidence_seq], train_labels, validation_split = 0.2,\n",
    "          epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10535,
     "status": "ok",
     "timestamp": 1713832682775,
     "user": {
      "displayName": "Yaqi Z",
      "userId": "12487690213530075120"
     },
     "user_tz": -60
    },
    "id": "-5M91Tt-U_Er",
    "outputId": "2170eb76-2878-4fee-a018-616e39126678"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186/186 [==============================] - 7s 38ms/step - loss: 0.3605 - accuracy: 0.8313\n",
      "Accuracy: 83.13%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate([dev_claim_seq, dev_evidence_seq], dev_labels)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c7-jJWZv9qMi"
   },
   "source": [
    "### Evaluation ###\n",
    "\n",
    "The details of evaluation can bee seen at https://colab.research.google.com/drive/1QPrZZIAHhJSoKdOt45ULcuHIytDHC5Vp?usp=sharing"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
